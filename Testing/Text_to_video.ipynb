{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key insights from the paper:\n",
      "Comprehensive Health Data Analysis for Early\n",
      "Dementia Diagnosis: A Machine Learning Approach\n",
      "Jaimin Salviâˆ—, Aagam Shahâ€ \n",
      "âˆ—Department of Computer Science, Nirma University, India\n",
      "â€ Department of Computer Science, Nirma University, India\n",
      "Abstractâ€”This paper investigates the application of machine\n",
      "learning (ML) models to predict dementia diagnosis using a\n",
      "comprehensive health dataset\n",
      " The dataset includes key health-\n",
      "related features such as diabetic status, heart rate, blood oxygen\n",
      "levels, body temperature, cognitive test scores, and lifestyle\n",
      "factors\n",
      " We employed ML techniques to predict dementia onset,\n",
      "leveraging algorithms such as support vector machines (SVM)\n",
      "and logistic regression\n",
      " Our findings demonstrate that ML\n",
      "models, particularly SVM and logistic regression, can effectively\n",
      "identify key predictors and achieve substantial accuracy in\n",
      "dementia prediction\n",
      " The primary aim of this study is to validate\n",
      "the performance of ML models in detecting dementia at an early\n",
      "stage and to identify the most influential health and cognitive\n",
      "factors contributing to dementia risk\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# ðŸ” Example Usage\n",
    "\n",
    "pdf_text = extract_text_from_pdf(\"Comprehensive Health Data Analysis for Early Dementia Diagnosis A Machine Learning Approach.pdf\")\n",
    "summary = \"Key insights from the paper:\\n\" + \"\\n\".join(pdf_text.split(\".\")[:5])  # Take first 5 sentences\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "\n",
    "tts = gTTS(summary, lang=\"en\")\n",
    "tts.save(\"audio.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video research_paper_reel.mp4.\n",
      "MoviePy - Writing audio in research_paper_reelTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video research_paper_reel.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready research_paper_reel.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import moviepy.config as mp_config\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Function to convert MP3 to WAV\n",
    "def convert_mp3_to_wav(mp3_file, wav_file):\n",
    "    audio = AudioSegment.from_mp3(mp3_file)\n",
    "    audio.export(wav_file, format=\"wav\")\n",
    "\n",
    "# Function to extract text and timing info from audio using SpeechRecognition\n",
    "def extract_text_from_audio_with_timestamps(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio = sr.AudioFile(audio_file)\n",
    "    \n",
    "    with audio as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        \n",
    "    try:\n",
    "        # Using Google Web Speech API with timestamps enabled\n",
    "        result = recognizer.recognize_google(audio_data, show_all=True)\n",
    "        \n",
    "        # Extract the words and their timings\n",
    "        words_with_timestamps = []\n",
    "        if 'alternative' in result:\n",
    "            for alternative in result['alternative']:\n",
    "                if 'timestamps' in alternative:\n",
    "                    for word_info in alternative['timestamps']:\n",
    "                        word, start_time, end_time = word_info\n",
    "                        words_with_timestamps.append((word, start_time, end_time))\n",
    "        return words_with_timestamps\n",
    "    except sr.UnknownValueError:\n",
    "        return []\n",
    "    except sr.RequestError:\n",
    "        return []\n",
    "\n",
    "# Convert MP3 to WAV\n",
    "audio_file_mp3 = \"audio.mp3\"\n",
    "audio_file_wav = \"audio.wav\"\n",
    "convert_mp3_to_wav(audio_file_mp3, audio_file_wav)\n",
    "\n",
    "# Extract text and timings from the audio file (now in WAV format)\n",
    "words_with_timestamps = extract_text_from_audio_with_timestamps(audio_file_wav)\n",
    "\n",
    "# Create text clips based on extracted words and timestamps\n",
    "text_clips = []\n",
    "for word, start_time, end_time in words_with_timestamps:\n",
    "    text_clip = TextClip(\n",
    "        txt=word,\n",
    "        font=\"Arial-Bold\",\n",
    "        fontsize=24,\n",
    "        color=\"white\"\n",
    "    ).set_duration(end_time - start_time).set_position(\"center\").set_start(start_time).crossfadein(0.5)\n",
    "    text_clips.append(text_clip)\n",
    "\n",
    "# Add images as background or for context (you can adjust images' visibility time)\n",
    "image1 = ImageClip(\"./Test_Image/research_image1.png\").set_duration(60).resize(width=720).set_position(\"center\")\n",
    "\n",
    "# Load audio file (now in WAV format)\n",
    "audio = AudioFileClip(audio_file_wav)\n",
    "\n",
    "# Combine everything (video, images, and text clips)\n",
    "final_video = CompositeVideoClip([image1] + text_clips)\n",
    "final_video = final_video.set_audio(audio).set_duration(audio.duration)\n",
    "\n",
    "# Export the final video\n",
    "final_video.write_videofile(\"research_paper_reel.mp4\", fps=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key insights from the paper comprehensive health data analysis for early dementia diagnosis a machine learning approach jimin Salvi asterisk operator aagam Shah asteris Cooperative department of computer science Nirma University India department of computer science Nirma University India abstract this paper investigates the application of machine learning ml models to predict dementia diagnosis using a comprehensive health data set the data set includes ki health related features such as diabetic status heart rate blood oxygen levels body temperature cognitive test scores and Lifestyle factors we employed ml techniques to predict debenture one set algorithm support vector machines SVM and Logistic regression of finding demonstrate that ml models particularly SVM and Logistic regression prediction the primary aim of the study is to validate the performance of ml models in detecting dimension at an early stage in to identify the most influential health and cognitive factors contributing to debenture risk\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "filename = \"audio.wav\"\n",
    "r = sr.Recognizer()\n",
    "with sr.AudioFile(filename) as source:\n",
    "    # listen for the data (load audio to memory)\n",
    "    audio_data = r.record(source)\n",
    "    # recognize (convert from speech to text)\n",
    "    text = r.recognize_google(audio_data)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video final_video_with_text_and_audio.mp4.\n",
      "MoviePy - Writing audio in final_video_with_text_and_audioTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video final_video_with_text_and_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready final_video_with_text_and_audio.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import pyttsx3\n",
    "from moviepy.editor import VideoFileClip, TextClip, CompositeVideoClip, AudioFileClip\n",
    "import time\n",
    "\n",
    "# Step 1: Create the audio (this part was done earlier)\n",
    "extracted_text = \"\"\"\n",
    "Key insights from the paper. \n",
    "Comprehensive Health Data Analysis for Early \n",
    "Dementia Diagnosis: A Machine Learning Approach.\n",
    "Jaimin Salvi, Aagam Shah\n",
    "Department of Computer Science, Nirma University, India. \n",
    "Abstractâ€”This paper investigates the application of machine \n",
    "learning (ML) models to predict dementia diagnosis using a \n",
    "comprehensive health dataset. \n",
    "The dataset includes key health-related features such as diabetic status, \n",
    "heart rate, blood oxygen levels, body temperature, cognitive test scores, \n",
    "and lifestyle factors. \n",
    "We employed ML techniques to predict dementia onset, leveraging \n",
    "algorithms such as support vector machines (SVM) and logistic regression. \n",
    "Our findings demonstrate that ML models, particularly SVM and logistic \n",
    "regression, can effectively identify key predictors and achieve substantial \n",
    "accuracy in dementia prediction. \n",
    "The primary aim of this study is to validate the performance of ML models \n",
    "in detecting dementia at an early stage and to identify the most influential \n",
    "health and cognitive factors contributing to dementia risk.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the TTS engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set properties (rate and volume)\n",
    "engine.setProperty('rate', 160)  # Adjust rate\n",
    "engine.setProperty('volume', 1)  # Full volume\n",
    "\n",
    "# Save audio to file\n",
    "audio_file = \"output_audio_with_pauses.mp3\"\n",
    "engine.save_to_file(extracted_text, audio_file)\n",
    "engine.runAndWait()\n",
    "\n",
    "# Step 2: Create the text clips\n",
    "text_chunks = [\n",
    "    (\"Key insights from the paper.\", 0, 4),\n",
    "    (\"Comprehensive Health Data Analysis for Early Dementia Diagnosis: A Machine Learning Approach.\", 4, 9),\n",
    "    (\"Jaimin Salvi, Aagam Shah\", 9, 13),\n",
    "    (\"Department of Computer Science, Nirma University, India.\", 13, 17),\n",
    "    (\"Abstractâ€”This paper investigates the application of machine learning (ML) models to predict dementia diagnosis using a comprehensive health dataset.\", 17, 23),\n",
    "    (\"The dataset includes key health-related features such as diabetic status, heart rate, blood oxygen levels, body temperature, cognitive test scores, and lifestyle factors.\", 23, 30),\n",
    "    (\"We employed ML techniques to predict dementia onset, leveraging algorithms such as support vector machines (SVM) and logistic regression.\", 30, 37),\n",
    "    (\"Our findings demonstrate that ML models, particularly SVM and logistic regression, can effectively identify key predictors and achieve substantial accuracy in dementia prediction.\", 37, 45),\n",
    "    (\"The primary aim of this study is to validate the performance of ML models in detecting dementia at an early stage and to identify the most influential health and cognitive factors contributing to dementia risk.\", 45, 55)\n",
    "]\n",
    "\n",
    "# Step 3: Create text clips and sync with audio\n",
    "text_clips = []\n",
    "\n",
    "for text, start_time, end_time in text_chunks:\n",
    "    text_clip = TextClip(\n",
    "        txt=text,\n",
    "        font=\"Arial-Bold\",\n",
    "        fontsize=24,\n",
    "        color=\"white\",\n",
    "        align=\"center\",\n",
    "        size=(720, 480)\n",
    "    ).set_duration(end_time - start_time).set_start(start_time).set_position(\"center\")\n",
    "    text_clips.append(text_clip)\n",
    "\n",
    "# Step 4: Add the audio\n",
    "audio = AudioFileClip(audio_file)\n",
    "\n",
    "# Step 5: Combine text clips with audio in a final video\n",
    "final_video = CompositeVideoClip(text_clips)\n",
    "final_video = final_video.set_audio(audio).set_duration(audio.duration)\n",
    "\n",
    "# Step 6: Export the final video\n",
    "final_video.write_videofile(\"final_video_with_text_and_audio.mp4\", fps=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "# Load the Whisper model (small version, but you can use large for better accuracy)\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Transcribe the audio and get subtitles with timestamps\n",
    "def transcribe_audio(audio_file):\n",
    "    result = model.transcribe(audio_file, word_timestamps=True)\n",
    "    return result[\"segments\"]\n",
    "\n",
    "# Extracted subtitles with timestamps\n",
    "audio_file = \"audio.mp3\"\n",
    "subtitles = transcribe_audio(audio_file)\n",
    "\n",
    "# Convert the result into .srt format\n",
    "def save_srt(subtitles, output_file):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for i, segment in enumerate(subtitles):\n",
    "            start_time = segment[\"start\"]\n",
    "            end_time = segment[\"end\"]\n",
    "            text = segment[\"text\"]\n",
    "            start_time_str = format_time(start_time)\n",
    "            end_time_str = format_time(end_time)\n",
    "            f.write(f\"{i + 1}\\n\")\n",
    "            f.write(f\"{start_time_str} --> {end_time_str}\\n\")\n",
    "            f.write(f\"{text}\\n\\n\")\n",
    "\n",
    "# Format the time to the correct .srt format (HH:MM:SS,MS)\n",
    "def format_time(seconds):\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    ms = int((s - int(s)) * 1000)\n",
    "    return f\"{int(h):02}:{int(m):02}:{int(s):02},{ms:03}\"\n",
    "\n",
    "# Save the subtitles to an SRT file\n",
    "save_srt(subtitles, \"output_subtitles.srt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video final_video_with_text_and_audio.mp4.\n",
      "MoviePy - Writing audio in final_video_with_text_and_audioTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video final_video_with_text_and_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready final_video_with_text_and_audio.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, TextClip, CompositeVideoClip\n",
    "import pysrt\n",
    "\n",
    "# Load video\n",
    "video = VideoFileClip(\"research_paper_reel.mp4\")\n",
    "\n",
    "# Load the .srt file\n",
    "subtitles_file = \"output_subtitles.srt\"\n",
    "subs = pysrt.open(subtitles_file)\n",
    "\n",
    "# Function to create TextClip for each subtitle\n",
    "def create_subtitle_clip(subtitle, font=\"Arial\", fontsize=24, color=\"white\"):\n",
    "    # Create a TextClip for the subtitle\n",
    "    return TextClip(subtitle.text, font=font, fontsize=fontsize, color=color, bg_color=\"black\", size=video.size).set_position(('center', 'bottom')).set_duration(subtitle.duration.seconds + subtitle.duration.milliseconds / 1000).set_start(subtitle.start.seconds + subtitle.start.milliseconds / 1000)\n",
    "\n",
    "# Create a list of subtitle clips\n",
    "subtitle_clips = [create_subtitle_clip(sub) for sub in subs]\n",
    "\n",
    "# Add subtitle clips to the video\n",
    "final_video = CompositeVideoClip([video] + subtitle_clips)\n",
    "\n",
    "# Export the final video with subtitles\n",
    "final_video.write_videofile(\"final_video_with_text_and_audio.mp4\", fps=24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.42G/1.42G [22:13<00:00, 1.15MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' Key insights from the paper Comprehensive health data analysis for early Dementia diagnosis A machine learning approach Jamin Salvi asterisk operator Agam Shah asterisk operator Department of Computer Science Nirma University India Department of Computer Science Nirma University India abstract This paper investigates the application of machine learning milliliter Models to predict dementia diagnosis using a Comprehensive health data set The data set includes key health related features such as Diabetic status Heart rate Blood oxygen Levels Body temperature Cognitive test scores And lifestyle Factors We employed milliliter techniques to predict dementia onset Leveraging algorithms such as support vector machines SVM And logistic regression Our findings demonstrate that milliliter Models Particularly SVM and logistic regression Can effectively identify key predictors and achieve Substantial accuracy in Dementia prediction The primary aim of this study is to validate The performance of milliliter models in detecting dementia at an Early stage and to identify the most influential health and Cognitive factors contributing to dementia risk', 'segments': [{'id': 0, 'seek': 0, 'start': 0.0, 'end': 2.22, 'text': ' Key insights from the paper', 'tokens': [50364, 12759, 14310, 490, 264, 3035, 50464], 'temperature': 0.0, 'avg_logprob': -0.3076466573795802, 'compression_ratio': 1.75, 'no_speech_prob': 0.27415767312049866, 'words': [{'word': ' Key', 'start': 0.0, 'end': 0.42, 'probability': 0.6325432062149048}, {'word': ' insights', 'start': 0.42, 'end': 0.98, 'probability': 0.7195944786071777}, {'word': ' from', 'start': 0.98, 'end': 1.28, 'probability': 0.995627760887146}, {'word': ' the', 'start': 1.28, 'end': 1.5, 'probability': 0.9955435395240784}, {'word': ' paper', 'start': 1.5, 'end': 2.22, 'probability': 0.9881762266159058}]}, {'id': 1, 'seek': 0, 'start': 2.22, 'end': 5.72, 'text': ' Comprehensive health data analysis for early', 'tokens': [50464, 2432, 40128, 2953, 1585, 1412, 5215, 337, 2440, 50641], 'temperature': 0.0, 'avg_logprob': -0.3076466573795802, 'compression_ratio': 1.75, 'no_speech_prob': 0.27415767312049866, 'words': [{'word': ' Comprehensive', 'start': 2.22, 'end': 3.28, 'probability': 0.8522834380467733}, {'word': ' health', 'start': 3.28, 'end': 3.64, 'probability': 0.5459597706794739}, {'word': ' data', 'start': 3.64, 'end': 4.06, 'probability': 0.9945386052131653}, {'word': ' analysis', 'start': 4.06, 'end': 4.64, 'probability': 0.9965159893035889}, {'word': ' for', 'start': 4.64, 'end': 5.04, 'probability': 0.9947131276130676}, {'word': ' early', 'start': 5.04, 'end': 5.72, 'probability': 0.9858627915382385}]}, {'id': 2, 'seek': 0, 'start': 5.72, 'end': 7.52, 'text': ' Dementia diagnosis', 'tokens': [50641, 413, 1712, 654, 15217, 50741], 'temperature': 0.0, 'avg_logprob': -0.3076466573795802, 'compression_ratio': 1.75, 'no_speech_prob': 0.27415767312049866, 'words': [{'word': ' Dementia', 'start': 5.72, 'end': 6.6, 'probability': 0.8926046093304952}, {'word': ' diagnosis', 'start': 6.6, 'end': 7.52, 'probability': 0.9341883659362793}]}, {'id': 3, 'seek': 0, 'start': 7.52, 'end': 9.74, 'text': ' A machine learning approach', 'tokens': [50741, 316, 3479, 2539, 3109, 50841], 'temperature': 0.0, 'avg_logprob': -0.3076466573795802, 'compression_ratio': 1.75, 'no_speech_prob': 0.27415767312049866, 'words': [{'word': ' A', 'start': 7.52, 'end': 8.02, 'probability': 0.49483853578567505}, {'word': ' machine', 'start': 8.02, 'end': 8.42, 'probability': 0.9057841300964355}, {'word': ' learning', 'start': 8.42, 'end': 8.8, 'probability': 0.9486314058303833}, {'word': ' approach', 'start': 8.8, 'end': 9.74, 'probability': 0.9948689937591553}]}, {'id': 4, 'seek': 0, 'start': 9.74, 'end': 12.54, 'text': ' Jamin Salvi asterisk operator', 'tokens': [50841, 508, 7428, 5996, 4917, 257, 3120, 7797, 12973, 50991], 'temperature': 0.0, 'avg_logprob': -0.3076466573795802, 'compression_ratio': 1.75, 'no_speech_prob': 0.27415767312049866, 'words': [{'word': ' Jamin', 'start': 9.74, 'end': 10.46, 'probability': 0.6038406789302826}, {'word': ' Salvi', 'start': 10.46, 'end': 10.98, 'probability': 0.952986478805542}, {'word': ' asterisk', 'start': 10.98, 'end': 11.68, 'probability': 0.6753253750503063}, {'word': ' operator', 'start': 11.68, 'end': 12.54, 'probability': 0.9358288645744324}]}, {'id': 5, 'seek': 0, 'start': 12.54, 'end': 17.74, 'text': ' Agam Shah asterisk operator Department of Computer Science', 'tokens': [50991, 2725, 335, 21159, 257, 3120, 7797, 12973, 5982, 295, 22289, 8976, 51241], 'temperature': 0.0, 'avg_logprob': -0.3076466573795802, 'compression_ratio': 1.75, 'no_speech_prob': 0.27415767312049866, 'words': [{'word': ' Agam', 'start': 12.54, 'end': 13.18, 'probability': 0.8179944455623627}, {'word': ' Shah', 'start': 13.18, 'end': 13.64, 'probability': 0.9009149074554443}, {'word': ' asterisk', 'start': 13.64, 'end': 14.78, 'probability': 0.9659239848454794}, {'word': ' operator', 'start': 14.78, 'end': 15.32, 'probability': 0.9830440878868103}, {'word': ' Department', 'start': 15.32, 'end': 16.0, 'probability': 0.8281176090240479}, {'word': ' of', 'start': 16.0, 'end': 16.34, 'probability': 0.9964569211006165}, {'word': ' Computer', 'start': 16.34, 'end': 16.86, 'probability': 0.543382465839386}, {'word': ' Science', 'start': 16.86, 'end': 17.74, 'probability': 0.940441906452179}]}, {'id': 6, 'seek': 0, 'start': 17.74, 'end': 22.84, 'text': ' Nirma University India Department of Computer Science', 'tokens': [51241, 44813, 1696, 3535, 5282, 5982, 295, 22289, 8976, 51506], 'temperature': 0.0, 'avg_logprob': -0.3076466573795802, 'compression_ratio': 1.75, 'no_speech_prob': 0.27415767312049866, 'words': [{'word': ' Nirma', 'start': 17.74, 'end': 18.38, 'probability': 0.6795787513256073}, {'word': ' University', 'start': 18.38, 'end': 19.46, 'probability': 0.9184233546257019}, {'word': ' India', 'start': 19.46, 'end': 20.32, 'probability': 0.7068161368370056}, {'word': ' Department', 'start': 20.32, 'end': 21.24, 'probability': 0.9565859436988831}, {'word': ' of', 'start': 21.24, 'end': 21.62, 'probability': 0.9974982142448425}, {'word': ' Computer', 'start': 21.62, 'end': 22.14, 'probability': 0.9692246317863464}, {'word': ' Science', 'start': 22.14, 'end': 22.84, 'probability': 0.9824177622795105}]}, {'id': 7, 'seek': 0, 'start': 22.84, 'end': 26.54, 'text': ' Nirma University India abstract', 'tokens': [51506, 44813, 1696, 3535, 5282, 12649, 51691], 'temperature': 0.0, 'avg_logprob': -0.3076466573795802, 'compression_ratio': 1.75, 'no_speech_prob': 0.27415767312049866, 'words': [{'word': ' Nirma', 'start': 22.84, 'end': 23.52, 'probability': 0.9773841202259064}, {'word': ' University', 'start': 23.52, 'end': 24.6, 'probability': 0.9931889772415161}, {'word': ' India', 'start': 24.6, 'end': 25.56, 'probability': 0.9856162071228027}, {'word': ' abstract', 'start': 25.56, 'end': 26.54, 'probability': 0.15620099008083344}]}, {'id': 8, 'seek': 2654, 'start': 26.54, 'end': 30.28, 'text': ' This paper investigates the application of machine', 'tokens': [50366, 639, 3035, 4557, 1024, 264, 3861, 295, 3479, 50566], 'temperature': 0.0, 'avg_logprob': -0.22805301622412671, 'compression_ratio': 1.502415458937198, 'no_speech_prob': 0.6757828593254089, 'words': [{'word': ' This', 'start': 26.54, 'end': 27.14, 'probability': 0.48950546979904175}, {'word': ' paper', 'start': 27.14, 'end': 27.62, 'probability': 0.9915502071380615}, {'word': ' investigates', 'start': 27.62, 'end': 28.52, 'probability': 0.9951601624488831}, {'word': ' the', 'start': 28.52, 'end': 28.82, 'probability': 0.9917019009590149}, {'word': ' application', 'start': 28.82, 'end': 29.42, 'probability': 0.9891170859336853}, {'word': ' of', 'start': 29.42, 'end': 29.7, 'probability': 0.9986566305160522}, {'word': ' machine', 'start': 29.7, 'end': 30.28, 'probability': 0.8252612352371216}]}, {'id': 9, 'seek': 2654, 'start': 30.28, 'end': 31.42, 'text': ' learning', 'tokens': [50566, 2539, 50616], 'temperature': 0.0, 'avg_logprob': -0.22805301622412671, 'compression_ratio': 1.502415458937198, 'no_speech_prob': 0.6757828593254089, 'words': [{'word': ' learning', 'start': 30.28, 'end': 31.42, 'probability': 0.8425037860870361}]}, {'id': 10, 'seek': 2654, 'start': 31.42, 'end': 32.64, 'text': ' milliliter', 'tokens': [50616, 1728, 388, 1681, 50666], 'temperature': 0.0, 'avg_logprob': -0.22805301622412671, 'compression_ratio': 1.502415458937198, 'no_speech_prob': 0.6757828593254089, 'words': [{'word': ' milliliter', 'start': 31.42, 'end': 32.64, 'probability': 0.7029049197832743}]}, {'id': 11, 'seek': 2654, 'start': 32.64, 'end': 36.48, 'text': ' Models to predict dementia diagnosis using a', 'tokens': [50666, 6583, 1625, 281, 6069, 31734, 15217, 1228, 257, 50866], 'temperature': 0.0, 'avg_logprob': -0.22805301622412671, 'compression_ratio': 1.502415458937198, 'no_speech_prob': 0.6757828593254089, 'words': [{'word': ' Models', 'start': 32.64, 'end': 33.26, 'probability': 0.5217118971049786}, {'word': ' to', 'start': 33.26, 'end': 33.56, 'probability': 0.9903970956802368}, {'word': ' predict', 'start': 33.56, 'end': 33.9, 'probability': 0.9802489876747131}, {'word': ' dementia', 'start': 33.9, 'end': 34.56, 'probability': 0.9258385300636292}, {'word': ' diagnosis', 'start': 34.56, 'end': 35.3, 'probability': 0.9640357494354248}, {'word': ' using', 'start': 35.3, 'end': 35.76, 'probability': 0.9861630797386169}, {'word': ' a', 'start': 35.76, 'end': 36.48, 'probability': 0.9715259671211243}]}, {'id': 12, 'seek': 2654, 'start': 36.48, 'end': 38.54, 'text': ' Comprehensive health data set', 'tokens': [50866, 2432, 40128, 2953, 1585, 1412, 992, 50966], 'temperature': 0.0, 'avg_logprob': -0.22805301622412671, 'compression_ratio': 1.502415458937198, 'no_speech_prob': 0.6757828593254089, 'words': [{'word': ' Comprehensive', 'start': 36.48, 'end': 37.34, 'probability': 0.8271934489409128}, {'word': ' health', 'start': 37.34, 'end': 37.72, 'probability': 0.4650534987449646}, {'word': ' data', 'start': 37.72, 'end': 38.2, 'probability': 0.8004112839698792}, {'word': ' set', 'start': 38.2, 'end': 38.54, 'probability': 0.9888288378715515}]}, {'id': 13, 'seek': 2654, 'start': 38.54, 'end': 42.4, 'text': ' The data set includes key health related features such as', 'tokens': [50966, 440, 1412, 992, 5974, 2141, 1585, 4077, 4122, 1270, 382, 51166], 'temperature': 0.0, 'avg_logprob': -0.22805301622412671, 'compression_ratio': 1.502415458937198, 'no_speech_prob': 0.6757828593254089, 'words': [{'word': ' The', 'start': 38.54, 'end': 39.02, 'probability': 0.08245623111724854}, {'word': ' data', 'start': 39.02, 'end': 39.44, 'probability': 0.9513468146324158}, {'word': ' set', 'start': 39.44, 'end': 39.64, 'probability': 0.994317352771759}, {'word': ' includes', 'start': 39.64, 'end': 40.16, 'probability': 0.9929798245429993}, {'word': ' key', 'start': 40.16, 'end': 40.52, 'probability': 0.9836261868476868}, {'word': ' health', 'start': 40.52, 'end': 40.84, 'probability': 0.9908389449119568}, {'word': ' related', 'start': 40.84, 'end': 41.28, 'probability': 0.8240084052085876}, {'word': ' features', 'start': 41.28, 'end': 41.8, 'probability': 0.986644446849823}, {'word': ' such', 'start': 41.8, 'end': 42.14, 'probability': 0.9209571480751038}, {'word': ' as', 'start': 42.14, 'end': 42.4, 'probability': 0.9982191920280457}]}, {'id': 14, 'seek': 2654, 'start': 42.4, 'end': 43.96, 'text': ' Diabetic status', 'tokens': [51166, 8789, 455, 3532, 6558, 51216], 'temperature': 0.0, 'avg_logprob': -0.22805301622412671, 'compression_ratio': 1.502415458937198, 'no_speech_prob': 0.6757828593254089, 'words': [{'word': ' Diabetic', 'start': 42.4, 'end': 43.02, 'probability': 0.6505063449343046}, {'word': ' status', 'start': 43.02, 'end': 43.96, 'probability': 0.9331158399581909}]}, {'id': 15, 'seek': 2654, 'start': 43.96, 'end': 45.14, 'text': ' Heart rate', 'tokens': [51216, 13569, 3314, 51266], 'temperature': 0.0, 'avg_logprob': -0.22805301622412671, 'compression_ratio': 1.502415458937198, 'no_speech_prob': 0.6757828593254089, 'words': [{'word': ' Heart', 'start': 43.96, 'end': 44.62, 'probability': 0.3743109703063965}, {'word': ' rate', 'start': 44.62, 'end': 45.14, 'probability': 0.9593861699104309}]}, {'id': 16, 'seek': 2654, 'start': 45.14, 'end': 46.54, 'text': ' Blood oxygen', 'tokens': [51266, 17428, 9169, 51366], 'temperature': 0.0, 'avg_logprob': -0.22805301622412671, 'compression_ratio': 1.502415458937198, 'no_speech_prob': 0.6757828593254089, 'words': [{'word': ' Blood', 'start': 45.14, 'end': 45.68, 'probability': 0.5038973689079285}, {'word': ' oxygen', 'start': 45.68, 'end': 46.54, 'probability': 0.9591498970985413}]}, {'id': 17, 'seek': 2654, 'start': 46.54, 'end': 47.82, 'text': ' Levels', 'tokens': [51366, 16872, 82, 51416], 'temperature': 0.0, 'avg_logprob': -0.22805301622412671, 'compression_ratio': 1.502415458937198, 'no_speech_prob': 0.6757828593254089, 'words': [{'word': ' Levels', 'start': 46.54, 'end': 47.82, 'probability': 0.7656649649143219}]}, {'id': 18, 'seek': 2654, 'start': 47.82, 'end': 48.58, 'text': ' Body temperature', 'tokens': [51416, 21329, 4292, 51466], 'temperature': 0.0, 'avg_logprob': -0.22805301622412671, 'compression_ratio': 1.502415458937198, 'no_speech_prob': 0.6757828593254089, 'words': [{'word': ' Body', 'start': 47.82, 'end': 48.22, 'probability': 0.9376599788665771}, {'word': ' temperature', 'start': 48.22, 'end': 48.76, 'probability': 0.9709972143173218}]}, {'id': 19, 'seek': 2654, 'start': 49.1, 'end': 50.98, 'text': ' Cognitive test scores', 'tokens': [51466, 383, 2912, 2187, 1500, 13444, 51566], 'temperature': 0.0, 'avg_logprob': -0.22805301622412671, 'compression_ratio': 1.502415458937198, 'no_speech_prob': 0.6757828593254089, 'words': [{'word': ' Cognitive', 'start': 49.1, 'end': 49.98, 'probability': 0.9899512330691019}, {'word': ' test', 'start': 49.98, 'end': 50.42, 'probability': 0.9319895505905151}, {'word': ' scores', 'start': 50.42, 'end': 50.98, 'probability': 0.9831069707870483}]}, {'id': 20, 'seek': 2654, 'start': 50.98, 'end': 52.44, 'text': ' And lifestyle', 'tokens': [51566, 400, 11716, 51666], 'temperature': 0.0, 'avg_logprob': -0.22805301622412671, 'compression_ratio': 1.502415458937198, 'no_speech_prob': 0.6757828593254089, 'words': [{'word': ' And', 'start': 50.98, 'end': 51.7, 'probability': 0.33027637004852295}, {'word': ' lifestyle', 'start': 51.7, 'end': 52.44, 'probability': 0.7647074460983276}]}, {'id': 21, 'seek': 2654, 'start': 52.44, 'end': 53.78, 'text': ' Factors', 'tokens': [51666, 33375, 830, 51716], 'temperature': 0.0, 'avg_logprob': -0.22805301622412671, 'compression_ratio': 1.502415458937198, 'no_speech_prob': 0.6757828593254089, 'words': [{'word': ' Factors', 'start': 52.44, 'end': 53.78, 'probability': 0.8927631676197052}]}, {'id': 22, 'seek': 5378, 'start': 53.78, 'end': 57.8, 'text': ' We employed milliliter techniques to predict dementia onset', 'tokens': [50366, 492, 20115, 1728, 388, 1681, 7512, 281, 6069, 31734, 34948, 50566], 'temperature': 0.0, 'avg_logprob': -0.10855362415313721, 'compression_ratio': 1.5902439024390245, 'no_speech_prob': 0.043423499912023544, 'words': [{'word': ' We', 'start': 53.78, 'end': 54.1, 'probability': 0.7824204564094543}, {'word': ' employed', 'start': 54.1, 'end': 54.58, 'probability': 0.9762060642242432}, {'word': ' milliliter', 'start': 54.58, 'end': 55.3, 'probability': 0.8901520570119222}, {'word': ' techniques', 'start': 55.3, 'end': 55.78, 'probability': 0.8424741625785828}, {'word': ' to', 'start': 55.78, 'end': 56.12, 'probability': 0.995101809501648}, {'word': ' predict', 'start': 56.12, 'end': 56.46, 'probability': 0.9985707998275757}, {'word': ' dementia', 'start': 56.46, 'end': 57.06, 'probability': 0.8999819159507751}, {'word': ' onset', 'start': 57.06, 'end': 57.8, 'probability': 0.9384417533874512}]}, {'id': 23, 'seek': 5378, 'start': 57.8, 'end': 61.82, 'text': ' Leveraging algorithms such as support vector machines', 'tokens': [50566, 441, 1054, 3568, 14642, 1270, 382, 1406, 8062, 8379, 50766], 'temperature': 0.0, 'avg_logprob': -0.10855362415313721, 'compression_ratio': 1.5902439024390245, 'no_speech_prob': 0.043423499912023544, 'words': [{'word': ' Leveraging', 'start': 57.8, 'end': 59.02, 'probability': 0.6703760325908661}, {'word': ' algorithms', 'start': 59.02, 'end': 59.72, 'probability': 0.9508705735206604}, {'word': ' such', 'start': 59.72, 'end': 60.08, 'probability': 0.9562366604804993}, {'word': ' as', 'start': 60.08, 'end': 60.34, 'probability': 0.9945809245109558}, {'word': ' support', 'start': 60.34, 'end': 60.72, 'probability': 0.8571340441703796}, {'word': ' vector', 'start': 60.72, 'end': 61.12, 'probability': 0.9563918709754944}, {'word': ' machines', 'start': 61.12, 'end': 61.82, 'probability': 0.9910649061203003}]}, {'id': 24, 'seek': 5378, 'start': 61.82, 'end': 62.82, 'text': ' SVM', 'tokens': [50766, 31910, 44, 50816], 'temperature': 0.0, 'avg_logprob': -0.10855362415313721, 'compression_ratio': 1.5902439024390245, 'no_speech_prob': 0.043423499912023544, 'words': [{'word': ' SVM', 'start': 61.82, 'end': 62.82, 'probability': 0.6913576722145081}]}, {'id': 25, 'seek': 5378, 'start': 63.5, 'end': 64.82000000000001, 'text': ' And logistic regression', 'tokens': [50816, 400, 3565, 3142, 24590, 50916], 'temperature': 0.0, 'avg_logprob': -0.10855362415313721, 'compression_ratio': 1.5902439024390245, 'no_speech_prob': 0.043423499912023544, 'words': [{'word': ' And', 'start': 63.5, 'end': 63.92, 'probability': 0.03471366688609123}, {'word': ' logistic', 'start': 63.92, 'end': 64.62, 'probability': 0.950464278459549}, {'word': ' regression', 'start': 64.62, 'end': 65.28, 'probability': 0.9972686767578125}]}, {'id': 26, 'seek': 5378, 'start': 65.34, 'end': 67.82000000000001, 'text': ' Our findings demonstrate that milliliter', 'tokens': [50916, 2621, 16483, 11698, 300, 1728, 388, 1681, 51066], 'temperature': 0.0, 'avg_logprob': -0.10855362415313721, 'compression_ratio': 1.5902439024390245, 'no_speech_prob': 0.043423499912023544, 'words': [{'word': ' Our', 'start': 65.34, 'end': 65.96, 'probability': 0.7456265687942505}, {'word': ' findings', 'start': 65.96, 'end': 66.44, 'probability': 0.9922816157341003}, {'word': ' demonstrate', 'start': 66.44, 'end': 67.16, 'probability': 0.973569393157959}, {'word': ' that', 'start': 67.16, 'end': 67.48, 'probability': 0.9930676221847534}, {'word': ' milliliter', 'start': 67.48, 'end': 68.14, 'probability': 0.9547562599182129}]}, {'id': 27, 'seek': 5378, 'start': 68.54, 'end': 68.82000000000001, 'text': ' Models', 'tokens': [51066, 6583, 1625, 51116], 'temperature': 0.0, 'avg_logprob': -0.10855362415313721, 'compression_ratio': 1.5902439024390245, 'no_speech_prob': 0.043423499912023544, 'words': [{'word': ' Models', 'start': 68.54, 'end': 69.2, 'probability': 0.5736508369445801}]}, {'id': 28, 'seek': 5378, 'start': 69.6, 'end': 72.98, 'text': ' Particularly SVM and logistic regression', 'tokens': [51116, 32281, 31910, 44, 293, 3565, 3142, 24590, 51316], 'temperature': 0.0, 'avg_logprob': -0.10855362415313721, 'compression_ratio': 1.5902439024390245, 'no_speech_prob': 0.043423499912023544, 'words': [{'word': ' Particularly', 'start': 69.6, 'end': 70.56, 'probability': 0.28052574396133423}, {'word': ' SVM', 'start': 70.56, 'end': 71.34, 'probability': 0.9611037969589233}, {'word': ' and', 'start': 71.34, 'end': 71.58, 'probability': 0.9455748796463013}, {'word': ' logistic', 'start': 71.58, 'end': 72.26, 'probability': 0.9418964087963104}, {'word': ' regression', 'start': 72.26, 'end': 72.98, 'probability': 0.995011568069458}]}, {'id': 29, 'seek': 5378, 'start': 72.98, 'end': 77.08, 'text': ' Can effectively identify key predictors and achieve', 'tokens': [51316, 1664, 8659, 5876, 2141, 6069, 830, 293, 4584, 51516], 'temperature': 0.0, 'avg_logprob': -0.10855362415313721, 'compression_ratio': 1.5902439024390245, 'no_speech_prob': 0.043423499912023544, 'words': [{'word': ' Can', 'start': 72.98, 'end': 73.66, 'probability': 0.6923487186431885}, {'word': ' effectively', 'start': 73.66, 'end': 74.6, 'probability': 0.9716278314590454}, {'word': ' identify', 'start': 74.6, 'end': 75.58, 'probability': 0.593265175819397}, {'word': ' key', 'start': 75.58, 'end': 75.94, 'probability': 0.993955671787262}, {'word': ' predictors', 'start': 75.94, 'end': 76.48, 'probability': 0.9958507418632507}, {'word': ' and', 'start': 76.48, 'end': 76.82, 'probability': 0.7803254723548889}, {'word': ' achieve', 'start': 76.82, 'end': 77.08, 'probability': 0.9887374639511108}]}, {'id': 30, 'seek': 5378, 'start': 77.08, 'end': 78.82, 'text': ' Substantial accuracy in', 'tokens': [51516, 42090, 394, 831, 14170, 294, 51616], 'temperature': 0.0, 'avg_logprob': -0.10855362415313721, 'compression_ratio': 1.5902439024390245, 'no_speech_prob': 0.043423499912023544, 'words': [{'word': ' Substantial', 'start': 77.08, 'end': 78.02, 'probability': 0.6683004548152288}, {'word': ' accuracy', 'start': 78.02, 'end': 78.66, 'probability': 0.9649009704589844}, {'word': ' in', 'start': 78.66, 'end': 79.32, 'probability': 0.9819130897521973}]}, {'id': 31, 'seek': 5378, 'start': 79.46, 'end': 80.78, 'text': ' Dementia prediction', 'tokens': [51616, 413, 1712, 654, 17630, 51716], 'temperature': 0.0, 'avg_logprob': -0.10855362415313721, 'compression_ratio': 1.5902439024390245, 'no_speech_prob': 0.043423499912023544, 'words': [{'word': ' Dementia', 'start': 79.46, 'end': 80.1, 'probability': 0.8128855923811594}, {'word': ' prediction', 'start': 80.1, 'end': 80.78, 'probability': 0.9691784977912903}]}, {'id': 32, 'seek': 8078, 'start': 80.78, 'end': 84.02, 'text': ' The primary aim of this study is to validate', 'tokens': [50366, 440, 6194, 5939, 295, 341, 2979, 307, 281, 29562, 50516], 'temperature': 0.0, 'avg_logprob': -0.13945099711418152, 'compression_ratio': 1.4761904761904763, 'no_speech_prob': 0.18693138659000397, 'words': [{'word': ' The', 'start': 80.78, 'end': 81.32, 'probability': 0.8713645935058594}, {'word': ' primary', 'start': 81.32, 'end': 81.84, 'probability': 0.9930063486099243}, {'word': ' aim', 'start': 81.84, 'end': 82.1, 'probability': 0.9978721141815186}, {'word': ' of', 'start': 82.1, 'end': 82.38, 'probability': 0.9985359907150269}, {'word': ' this', 'start': 82.38, 'end': 82.6, 'probability': 0.780302882194519}, {'word': ' study', 'start': 82.6, 'end': 82.92, 'probability': 0.997631311416626}, {'word': ' is', 'start': 82.92, 'end': 83.24, 'probability': 0.9968865513801575}, {'word': ' to', 'start': 83.24, 'end': 83.48, 'probability': 0.9984896183013916}, {'word': ' validate', 'start': 83.48, 'end': 84.02, 'probability': 0.9953567385673523}]}, {'id': 33, 'seek': 8078, 'start': 84.02, 'end': 88.56, 'text': ' The performance of milliliter models in detecting dementia at an', 'tokens': [50516, 440, 3389, 295, 1728, 388, 1681, 5245, 294, 40237, 31734, 412, 364, 50716], 'temperature': 0.0, 'avg_logprob': -0.13945099711418152, 'compression_ratio': 1.4761904761904763, 'no_speech_prob': 0.18693138659000397, 'words': [{'word': ' The', 'start': 84.02, 'end': 84.72, 'probability': 0.018288156017661095}, {'word': ' performance', 'start': 84.72, 'end': 85.26, 'probability': 0.9526005387306213}, {'word': ' of', 'start': 85.26, 'end': 85.68, 'probability': 0.9987447261810303}, {'word': ' milliliter', 'start': 85.68, 'end': 86.2, 'probability': 0.8574797113736471}, {'word': ' models', 'start': 86.2, 'end': 86.68, 'probability': 0.9807424545288086}, {'word': ' in', 'start': 86.68, 'end': 87.06, 'probability': 0.9842314720153809}, {'word': ' detecting', 'start': 87.06, 'end': 87.5, 'probability': 0.9911222457885742}, {'word': ' dementia', 'start': 87.5, 'end': 88.08, 'probability': 0.8924120664596558}, {'word': ' at', 'start': 88.08, 'end': 88.32, 'probability': 0.9862218499183655}, {'word': ' an', 'start': 88.32, 'end': 88.56, 'probability': 0.9976575374603271}]}, {'id': 34, 'seek': 8078, 'start': 88.56, 'end': 92.9, 'text': ' Early stage and to identify the most influential health and', 'tokens': [50716, 18344, 3233, 293, 281, 5876, 264, 881, 22215, 1585, 293, 50966], 'temperature': 0.0, 'avg_logprob': -0.13945099711418152, 'compression_ratio': 1.4761904761904763, 'no_speech_prob': 0.18693138659000397, 'words': [{'word': ' Early', 'start': 88.56, 'end': 89.0, 'probability': 0.006565257906913757}, {'word': ' stage', 'start': 89.0, 'end': 89.84, 'probability': 0.24088448286056519}, {'word': ' and', 'start': 89.84, 'end': 90.1, 'probability': 0.9077473878860474}, {'word': ' to', 'start': 90.1, 'end': 90.32, 'probability': 0.9826453924179077}, {'word': ' identify', 'start': 90.32, 'end': 90.92, 'probability': 0.9766877293586731}, {'word': ' the', 'start': 90.92, 'end': 91.2, 'probability': 0.989017128944397}, {'word': ' most', 'start': 91.2, 'end': 91.54, 'probability': 0.9844186305999756}, {'word': ' influential', 'start': 91.54, 'end': 92.18, 'probability': 0.9798151254653931}, {'word': ' health', 'start': 92.18, 'end': 92.6, 'probability': 0.9318193793296814}, {'word': ' and', 'start': 92.6, 'end': 92.9, 'probability': 0.9089058041572571}]}, {'id': 35, 'seek': 8078, 'start': 92.9, 'end': 96.52, 'text': ' Cognitive factors contributing to dementia risk', 'tokens': [50966, 383, 2912, 2187, 6771, 19270, 281, 31734, 3148, 51166], 'temperature': 0.0, 'avg_logprob': -0.13945099711418152, 'compression_ratio': 1.4761904761904763, 'no_speech_prob': 0.18693138659000397, 'words': [{'word': ' Cognitive', 'start': 92.9, 'end': 93.94, 'probability': 0.6714448115477959}, {'word': ' factors', 'start': 93.94, 'end': 94.46, 'probability': 0.953827977180481}, {'word': ' contributing', 'start': 94.46, 'end': 95.26, 'probability': 0.9387150406837463}, {'word': ' to', 'start': 95.26, 'end': 95.64, 'probability': 0.997445821762085}, {'word': ' dementia', 'start': 95.64, 'end': 96.1, 'probability': 0.8872715830802917}, {'word': ' risk', 'start': 96.1, 'end': 96.52, 'probability': 0.9823764562606812}]}], 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "audiofilename=\"audio.mp3\"\n",
    "import whisper #might take some time (approx 3- 5min depending on audio length)\n",
    "model = whisper.load_model(\"medium\")\n",
    "result = model.transcribe(audiofilename,word_timestamps=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
